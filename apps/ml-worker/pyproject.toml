[project]
name = "ml-worker"
version = "0.1.0"
description = "A dedicated FastAPI service for running heavy, self-hosted AI models like Whisper and Ollama."
readme = "README.md"
# Pinning to Python 3.12, which has stable support across the ML ecosystem.
requires-python = "~=3.12.12"

# For this service, all ML packages are core dependencies, not optional.
dependencies = [
    # --- Web Service Essentials ---
    "fastapi",
    "uvicorn",

    # --- Standard Utilities ---
    "httpx",
    "python-dotenv",
    "python-multipart",
    "soundfile", # For audio processing

    # --- AI & ML Libraries ---
    # Client for interacting with Ollama's OpenAI-compatible API
    "openai>=2.5.0",

    # Core deep learning frameworks
    "torch",
    "transformers",

    # Hugging Face ecosystem helpers for performance
    "accelerate",
    "bitsandbytes",
    "hf-xet",

    # High-performance local transcription libraries
    "whisperx",
    "openai-whisper"
]

[build-system]
requires = ["setuptools"]
build-backend = "setuptools.build_meta"